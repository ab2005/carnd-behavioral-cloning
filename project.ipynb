{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Deep Learning to Clone Driving Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission criterias\n",
    "\n",
    "### Quality of Code\n",
    "- Is the code functional?\n",
    "    - TODO The model provided can be used to successfully operate the simulation.\n",
    "- Is the code usable and readable?\n",
    "    - TODO The code in model.py uses a Python generator, if needed, to generate data for training rather than storing the training data in memory. \n",
    "    - TODO: The model.py code is clearly organized and comments are included where needed.\n",
    "    \n",
    "### Model Architecture and Training Strategy\n",
    "- Has an appropriate model architecture been employed for the task?\n",
    "    * TODO: The neural network uses convolution layers with appropriate filter sizes. \n",
    "    * TODO: Layers exist to introduce nonlinearity into the model. \n",
    "    * TODO: The data is normalized in the model.\n",
    "    \n",
    "- Has an attempt been made to reduce overfitting of the model?\n",
    "    - TODO: Train/validation/test splits have been used, and the model uses dropout layers or other methods to reduce overfitting.\n",
    "- Have the model parameters been tuned appropriately?\n",
    "    - TODO: Learning rate parameters are chosen with explanation, or an Adam optimizer is used.\n",
    "- Is the training data chosen appropriately?\n",
    "    - TODO: Training data has been chosen to induce the desired behavior in the simulation (i.e. keeping the car on the track).\n",
    "    \n",
    "### Architecture and Training Documentation\n",
    "- Is the solution design documented?\n",
    "    - The README thoroughly discusses the approach taken for deriving and designing a model architecture fit for solving the given problem.\n",
    "- Is the model architecture documented?\n",
    "    - The README provides sufficient details of the characteristics and qualities of the architecture, such as the type of model used, the number of layers, the size of each layer. Visualizations emphasizing particular qualities of the architecture are encouraged.\n",
    "- Is the creation of the training dataset and training process documented?\n",
    "    - The README describes how the model was trained and what the characteristics of the dataset are. Information such as how the dataset was generated and examples of images from the dataset should be included.\n",
    "    \n",
    "### Simulation\n",
    "- Is the car able to navigate correctly on test data?\n",
    "    - No tire may leave the drivable portion of the track surface. The car may not pop up onto ledges or roll over any surfaces that would otherwise be considered unsafe (if humans were in the vehicle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The list of files to submit:\n",
    "- model.py - The script used to create and train the model.\n",
    "- [drive.py](./drive.py) - The script to drive the car. You can feel free to resubmit the original drive.py or make modifications and submit your modified version.\n",
    "- model.json - The model architecture.\n",
    "- model.h5 - The model weights.\n",
    "- [README.md](./README.md) - explains the structure of your network and training approach. While we recommend using English for good practice, writing in any language is acceptable (reviewers will translate). There is no minimum word count so long as there are complete descriptions of the problems and the strategies. See the rubric for more details about the expectations.\n",
    "\n",
    "## 1. Files in this repo\n",
    "* `model.py`: Python script to import data, train model and save model.\n",
    "* `model.json`: Model architecture.\n",
    "* `model.h5`: Model weights (Large file, > 300MB).\n",
    "* `drive.py`: Python script that tells the car in the simulator how to drive\n",
    "* `data/`: file with training data\n",
    "    * Attributes such as 'steering angle' mapped to image paths in `driving_log.csv`.\n",
    "    * Images in `IMG/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "We used the same augmentation scheme as before. The augmentations applied were,\n",
    "#### Brightness augmentation: \n",
    "Changing brightness of the input image by scaling the V channel between .75 and 1.25 in the transformed HSV image.\n",
    "#### Using left and right camera images: \n",
    "To simulate the effect of car wandering off to the side, and recovering. We added a small angle .25 to the left camera and subtracted a small angle of 0.25 from the right camera. The main idea being the left camera has to move right to get to center, and right camera has to move left.\n",
    "#### Horizontal and vertical shifts:\n",
    "We randomly shifted the camera images horizontally to simulate the effect of car being at different positions on the lane, and added an offset corresponding to the shift to the steering angle. We also shifted the images vertically by a random number to simulate the effect of driving up or down a slope.\n",
    "#### Flipping: \n",
    "We also flipped images at random and changed the sign of the predicted angle to simulate driving in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We cropped the top 1/5 and bottom 25 pixels of the image, to remove the blue protion from the sky and hood of the car from the image. We further resized the image to 200 by 66 to match the NVIDIA’s image format. After augmentation and preprocessing the images generated from 1 image change as shown below.\n",
    "![](https://cdn-images-1.medium.com/max/800/1*vLCKp-VcTl9-3Dt2Lgj3aA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO show angles distributions with images ![](https://cdn-images-1.medium.com/max/800/1*FB9Z4a0MUKs0tsD9PE6eYQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Objective to building model\n",
    "The goal is the car to drive within the lane lines, so the model needs to recognise lane lines from the camera images.\n",
    "TODO: Copy the Nvidia pipeline. It works :) And it's not too complex.\n",
    "\n",
    "### Preprocessing\n",
    "Re-size the input image. I was able to size the image down by 2, reducing the number of pixels by 4. This really helped speed up model training and did not seem to impact the accuracy.\n",
    "\n",
    "### Warning!\n",
    "drive.py sends RGB images to the model; cv2.imread() reads images in BGR format!!!!\n",
    "Fixed the bug and passed the 2 sharp corners at the first attempt lol.\n",
    "\n",
    "\n",
    "### Starting out\n",
    "\n",
    "##### Train 3 images \n",
    "When you're starting out, pick three images from the .csv file, one with negative steering, one with straight, and one with right steering. Train a model with just those three images and see if you can get it to predict them correctly. This will tell you that your model is good and your turn-around time will be very quick. Then you can start adding more training data\n",
    "\n",
    "##### Train on provided data \n",
    "There is 8037 snapshots provided in the training set. Use it to train the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is [Github project](https://github.com/vxy10/P3-BehaviorCloning) of Vivek Yadav [post 1](https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.vbfk0tp2p) and [post 2](https://chatbotslife.com/learning-human-driving-behavior-using-nvidias-neural-network-model-and-image-augmentation-80399360efee#.xrn3giarp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/k5GWzzE1qaU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10d24e438>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"k5GWzzE1qaU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA keras model\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*lGH4OFwoUfj5pX2OWLhGkg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Should we use this? Not really sure if we need it...\n",
    "\n",
    "def get_nvidia_model(summary=True):\n",
    "    init = 'glorot_uniform'\n",
    "\n",
    "    if K.backend() == 'theano':\n",
    "        input_frame = Input(shape=(CONFIG['input_channels'], NVIDIA_H, NVIDIA_W))\n",
    "    else:\n",
    "        input_frame = Input(shape=(NVIDIA_H, NVIDIA_W, CONFIG['input_channels']))\n",
    "\n",
    "    # input normalization\n",
    "    x = Lambda(lambda z: z / 127.5 - 1.)(input_frame)\n",
    "\n",
    "    x = Convolution2D(24, 5, 5, border_mode='valid', subsample=(2, 2), init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Convolution2D(36, 5, 5, border_mode='valid', subsample=(2, 2), init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Convolution2D(48, 5, 5, border_mode='valid', subsample=(2, 2), init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Convolution2D(64, 3, 3, border_mode='valid', init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Convolution2D(64, 3, 3, border_mode='valid', init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(100, init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(10, init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    out = Dense(1, init=init)(x)\n",
    "\n",
    "    model = Model(input=input_frame, output=out)\n",
    "\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out [Comma.ai model](https://github.com/commaai/research/blob/master/train_steering_model.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/commaai/research/blob/master/train_steering_model.py\n",
    "\"\"\"\n",
    "Steering angle prediction model\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "from server import client_generator\n",
    "\n",
    "\n",
    "def gen(hwm, host, port):\n",
    "  for tup in client_generator(hwm=hwm, host=host, port=port):\n",
    "    X, Y, _ = tup\n",
    "    Y = Y[:, -1]\n",
    "    if X.shape[1] == 1:  # no temporal context\n",
    "      X = X[:, -1]\n",
    "    yield X, Y\n",
    "\n",
    "\n",
    "def get_model(time_len=1):\n",
    "  ch, row, col = 3, 160, 320  # camera format\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "            input_shape=(ch, row, col),\n",
    "            output_shape=(ch, row, col)))\n",
    "  model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\n",
    "  model.add(ELU())\n",
    "  model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "  model.add(ELU())\n",
    "  model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dropout(.2))\n",
    "  model.add(ELU())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Dropout(.5))\n",
    "  model.add(ELU())\n",
    "  model.add(Dense(1))\n",
    "\n",
    "  model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser(description='Steering angle model trainer')\n",
    "  parser.add_argument('--host', type=str, default=\"localhost\", help='Data server ip address.')\n",
    "  parser.add_argument('--port', type=int, default=5557, help='Port of server.')\n",
    "  parser.add_argument('--val_port', type=int, default=5556, help='Port of server for validation dataset.')\n",
    "  parser.add_argument('--batch', type=int, default=64, help='Batch size.')\n",
    "  parser.add_argument('--epoch', type=int, default=200, help='Number of epochs.')\n",
    "  parser.add_argument('--epochsize', type=int, default=10000, help='How many frames per epoch.')\n",
    "  parser.add_argument('--skipvalidate', dest='skipvalidate', action='store_true', help='Multiple path output.')\n",
    "  parser.set_defaults(skipvalidate=False)\n",
    "  parser.set_defaults(loadweights=False)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  model = get_model()\n",
    "  model.fit_generator(\n",
    "    gen(20, args.host, port=args.port),\n",
    "    samples_per_epoch=10000,\n",
    "    nb_epoch=args.epoch,\n",
    "    validation_data=gen(20, args.host, port=args.val_port),\n",
    "    nb_val_samples=1000\n",
    "  )\n",
    "  print(\"Saving model weights and configuration file.\")\n",
    "\n",
    "  if not os.path.exists(\"./outputs/steering_model\"):\n",
    "      os.makedirs(\"./outputs/steering_model\")\n",
    "\n",
    "  model.save_weights(\"./outputs/steering_model/steering_angle.keras\", True)\n",
    "  with open('./outputs/steering_model/steering_angle.json', 'w') as outfile:\n",
    "    json.dump(model.to_json(), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [NVIDIA model](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) \n",
    "\n",
    "<img src=\"https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/08/cnn-architecture.png\" width=\"400\">\n",
    "\n",
    "<p><center>NVIDIA model architecture, taken from [End to end learning for self-driving cars](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf)</center>\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*lGH4OFwoUfj5pX2OWLhGkg.png)\n",
    "We did not use dropouts or maxpooling in our architecture to remain true to NVIDIA model. \n",
    "\n",
    "We used exponential leaky units (ELU) for activation functions, because ELUs have smoother derivatives at zero, and hence are expected to be slighly better for predicted continuous values. We tested the model with rectified linear units (ReLU) too, and noticed minimal performance difference between the two networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TODO: provide correct model without dropouts\n",
    "\n",
    "we choose ELU() over ReLu because ELUs have [improved learning characteristics](https://arxiv.org/pdf/1511.07289v1.pdf) compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_12 (Lambda)               (None, 66, 200, 3)    0           lambda_input_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_31 (Convolution2D) (None, 31, 98, 24)    1824        lambda_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_41 (ELU)                     (None, 31, 98, 24)    0           convolution2d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_32 (Convolution2D) (None, 14, 47, 36)    21636       elu_41[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_42 (ELU)                     (None, 14, 47, 36)    0           convolution2d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_33 (Convolution2D) (None, 5, 22, 48)     43248       elu_42[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_43 (ELU)                     (None, 5, 22, 48)     0           convolution2d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_34 (Convolution2D) (None, 3, 20, 64)     27712       elu_43[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_44 (ELU)                     (None, 3, 20, 64)     0           convolution2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_35 (Convolution2D) (None, 1, 18, 64)     36928       elu_44[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_45 (ELU)                     (None, 1, 18, 64)     0           convolution2d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 1152)          0           elu_45[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 1164)          1342092     flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_46 (ELU)                     (None, 1164)          0           dense_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 100)           116500      elu_46[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_47 (ELU)                     (None, 100)           0           dense_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 50)            5050        elu_47[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_48 (ELU)                     (None, 50)            0           dense_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 10)            510         elu_48[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_49 (ELU)                     (None, 10)            0           dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 1)             11          elu_49[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_50 (ELU)                     (None, 1)             0           dense_20[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,595,511\n",
      "Trainable params: 1,595,511\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "def nvidia_model(summary=True):\n",
    "    init = 'he_normal'\n",
    "    model = Sequential()\n",
    "    # TODO:  checkif we can do mean subtraction\n",
    "    model.add(Lambda(lambda z: z / 127.5 - 1., input_shape=[66, 200, 3]))\n",
    "    \n",
    "    model.add(Convolution2D(24, 5, 5, border_mode='valid', subsample=(2, 2), init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Convolution2D(36, 5, 5, border_mode='valid', subsample=(2, 2), init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Convolution2D(48, 5, 5, border_mode='valid', subsample=(2, 2), init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='valid', subsample=(1, 1), init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='valid', subsample=(1, 1), init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1164, init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Dense(100, init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Dense(50, init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Dense(10, init=init))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Dense(1, init=init))\n",
    "    model.add(ELU())\n",
    "    \n",
    "    if summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = nvidia_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NVIDIA_H, NVIDIA_W = 66, 200\n",
    "\n",
    "CONFIG = {\n",
    "    'batchsize': 512,\n",
    "    'input_width': NVIDIA_W,\n",
    "    'input_height': NVIDIA_H,\n",
    "    'input_channels': 3,\n",
    "    'delta_correction': 0.25,\n",
    "    'augmentation_steer_sigma': 0.2,\n",
    "    'augmentation_value_min': 0.2,\n",
    "    'augmentation_value_max': 1.5,\n",
    "    'bias': 0.8,\n",
    "    'crop_height': range(20, 140)\n",
    "}\n",
    "\n",
    "def get_nvidia_model(summary=True):\n",
    "    init = 'glorot_uniform'\n",
    "\n",
    "    input_frame = Input(shape=(66, 200, 3))\n",
    "\n",
    "    # input normalization\n",
    "    ''' The first layer of the network performs image normalization. The normalizer is hard-coded \n",
    "    and is not adjusted in the learning process. \n",
    "    Performing normalization in the network allows the normalization scheme to be altered with \n",
    "    the network architecture, and to be accelerated via GPU processing.'''\n",
    "    x = Lambda(lambda z: z / 127.5 - 1.)(input_frame)\n",
    "\n",
    "    ''' The convolutional layers are designed to perform feature extraction, and are chosen empirically \n",
    "    through a series of experiments that vary layer configurations. \n",
    "    We then use strided convolutions in the first three convolutional layers with a 2×2 stride and a 5×5 kernel'''\n",
    "    x = Convolution2D(24, 5, 5, border_mode='valid', subsample=(2, 2), init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Convolution2D(36, 5, 5, border_mode='valid', subsample=(2, 2), init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Convolution2D(48, 5, 5, border_mode='valid', subsample=(2, 2), init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    '''We use a non-strided convolution with a 3×3 kernel size in the final two convolutional layers.'''\n",
    "    x = Convolution2D(64, 3, 3, border_mode='valid', init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Convolution2D(64, 3, 3, border_mode='valid', init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    ''' We follow the five convolutional layers with three fully connected layers, leading to a final \n",
    "    output control value which is the inverse-turning-radius. \n",
    "    The fully connected layers are designed to function as a controller for steering, but we noted \n",
    "    that by training the system end-to-end, it is not possible to make a clean break between which \n",
    "    parts of the network function primarily as feature extractor, and which serve as controller.'''\n",
    "    x = Dense(1164, init=init)(x)\n",
    "    x = ELU()(x)    \n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(100, init=init)(x)\n",
    "    x = ELU()(x)    \n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(50, init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(10, init=init)(x)\n",
    "    x = ELU()(x)\n",
    "    \n",
    "    out = Dense(1, init=init)(x)\n",
    "\n",
    "    model = Model(input=input_frame, output=out)\n",
    "\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 66, 200, 3)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 66, 200, 3)    0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 31, 98, 24)    1824        lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_19 (ELU)                     (None, 31, 98, 24)    0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 31, 98, 24)    0           elu_19[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 14, 47, 36)    21636       dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "elu_20 (ELU)                     (None, 14, 47, 36)    0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 14, 47, 36)    0           elu_20[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 5, 22, 48)     43248       dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "elu_21 (ELU)                     (None, 5, 22, 48)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 5, 22, 48)     0           elu_21[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 3, 20, 64)     27712       dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "elu_22 (ELU)                     (None, 3, 20, 64)     0           convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 3, 20, 64)     0           elu_22[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 1, 18, 64)     36928       dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "elu_23 (ELU)                     (None, 1, 18, 64)     0           convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 1, 18, 64)     0           elu_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 1152)          0           dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1164)          1342092     flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_24 (ELU)                     (None, 1164)          0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)             (None, 1164)          0           elu_24[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 100)           116500      dropout_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "elu_25 (ELU)                     (None, 100)           0           dense_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)             (None, 100)           0           elu_25[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 50)            5050        dropout_23[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "elu_26 (ELU)                     (None, 50)            0           dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)             (None, 50)            0           elu_26[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 10)            510         dropout_24[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "elu_27 (ELU)                     (None, 10)            0           dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 1)             11          elu_27[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 1,595,511\n",
      "Trainable params: 1,595,511\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_nvidia_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_steering_distribution(train_data):\n",
    "    \"\"\"\n",
    "    Visualize the training ground truth distribution \"as provided\"\n",
    "    :param train_data: list of udacity training data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_steering = np.float32(np.array(train_data)[:, 3])\n",
    "    plt.title('Steering angle distribution in training data')\n",
    "    plt.hist(train_steering, 100, normed=0, facecolor='green', alpha=0.75)\n",
    "    plt.ylabel('# frames'), plt.xlabel('steering angle')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_bias_parameter_effect(train_data):\n",
    "    \"\"\"\n",
    "    Visualize how the 'bias' parameter influences the ground truth distribution\n",
    "    :param train_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    biases = np.linspace(start=0., stop=1., num=5)\n",
    "    fig, axarray = plt.subplots(len(biases))\n",
    "    plt.suptitle('Effect of bias parameter on steering angle distribution', fontsize=14, fontweight='bold')\n",
    "    for i, ax in enumerate(axarray.ravel()):\n",
    "        b = biases[i]\n",
    "        x_batch, y_batch = load_data_batch(train_data, batchsize=1024, augment_data=True, bias=b)\n",
    "        ax.hist(y_batch, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "        ax.set_title('Bias: {:02f}'.format(b))\n",
    "        ax.axis([-1., 1., 0., 2.])\n",
    "    plt.tight_layout(pad=2, w_pad=0.5, h_pad=1.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e3972c44e8d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mload_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_data_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_train_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'load_data'"
     ]
    }
   ],
   "source": [
    "# NVIDIA model\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Flatten, Dense, Dropout, ELU, Lambda\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "NVIDIA_H, NVIDIA_W = 66, 200\n",
    "\n",
    "CONFIG = {\n",
    "    'batchsize': 512,\n",
    "    'input_width': NVIDIA_W,\n",
    "    'input_height': NVIDIA_H,\n",
    "    'input_channels': 3,\n",
    "    'delta_correction': 0.25,\n",
    "    'augmentation_steer_sigma': 0.2,\n",
    "    'augmentation_value_min': 0.2,\n",
    "    'augmentation_value_max': 1.5,\n",
    "    'bias': 0.8,\n",
    "    'crop_height': range(20, 140)\n",
    "}\n",
    "\n",
    "def preprocess(frame_bgr, verbose=False):\n",
    "    # set training images resized shape\n",
    "    h, w = CONFIG['input_height'], CONFIG['input_width']\n",
    "\n",
    "    # crop image (remove useless information)\n",
    "    frame_cropped = frame_bgr[CONFIG['crop_height'], :, :]\n",
    "\n",
    "    # resize image\n",
    "    frame_resized = cv2.resize(frame_cropped, dsize=(w, h))\n",
    "\n",
    "    # eventually change color space\n",
    "    if CONFIG['input_channels'] == 1:\n",
    "        frame_resized = np.expand_dims(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2YUV)[:, :, 0], 2)\n",
    "\n",
    "    if verbose:\n",
    "        plt.figure(1), plt.imshow(cv2.cvtColor(frame_bgr, code=cv2.COLOR_BGR2RGB))\n",
    "        plt.figure(2), plt.imshow(cv2.cvtColor(frame_cropped, code=cv2.COLOR_BGR2RGB))\n",
    "        plt.figure(3), plt.imshow(cv2.cvtColor(frame_resized, code=cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "\n",
    "    return frame_resized.astype('float32')\n",
    "\n",
    "\n",
    "def load_data_batch(data, batchsize=CONFIG['batchsize'], data_dir='data', augment_data=True, bias=0.5):\n",
    "    # set training images resized shape\n",
    "    h, w, c = CONFIG['input_height'], CONFIG['input_width'], CONFIG['input_channels']\n",
    "\n",
    "    # prepare output structures\n",
    "    X = np.zeros(shape=(batchsize, h, w, c), dtype=np.float32)\n",
    "    y_steer = np.zeros(shape=(batchsize,), dtype=np.float32)\n",
    "    y_throttle = np.zeros(shape=(batchsize,), dtype=np.float32)\n",
    "\n",
    "    # shuffle data\n",
    "    shuffled_data = shuffle(data)\n",
    "\n",
    "    loaded_elements = 0\n",
    "    while loaded_elements < batchsize:\n",
    "\n",
    "        ct_path, lt_path, rt_path, steer, throttle, brake, speed = shuffled_data.pop()\n",
    "\n",
    "        # cast strings to float32\n",
    "        steer = np.float32(steer)\n",
    "        throttle = np.float32(throttle)\n",
    "'''\n",
    "One predominant problem in steering prediction is that even if the model predicts really really well, \n",
    "the model is bound to make a non zero error on it’s prediction. This means that over time the model \n",
    "will drift off from center. Since we are only training the model to drive correctly and not training \n",
    "the model how to recover (something that we humans know instinctively), the model is handicapped.\n",
    "\n",
    "To offset this problem, we choose images randomly between left, right or center images by doing \n",
    "the following. The approach is to add / subtract a static offset from the angle when choosing \n",
    "the left / right image. To get a smoother drive, we can add / subtract an offset that’s weighted \n",
    "by the magnitude of the angle. But this biases the model towards zeros again and is a design tradeoff. \n",
    "I chose the simple static offsets. This simulates the model drifting off the center and trains the model \n",
    "to recover from it’s mistakes. This also gives us a lot more data to work with than by just using the \n",
    "center images alone.\n",
    "img_choice = np.random.randint(3)\n",
    "if img_choice == 0:\n",
    "    img_path = os.path.join(PATH, df.left.iloc[idx].strip())\n",
    "    angle += OFF_CENTER_IMG\n",
    "elif img_choice == 1:\n",
    "    img_path = os.path.join(PATH, df.center.iloc[idx].strip())\n",
    "else:\n",
    "    img_path = os.path.join(PATH, df.right.iloc[idx].strip())\n",
    "    angle -= OFF_CENTER_IMG\n",
    "'''\n",
    "        # randomly choose which camera to use among (central, left, right)\n",
    "        # in case the chosen camera is not the frontal one, adjust steer accordingly\n",
    "        delta_correction = CONFIG['delta_correction']\n",
    "        camera = random.choice(['frontal', 'left', 'right'])\n",
    "        if camera == 'frontal':\n",
    "            frame = preprocess(cv2.imread(join(data_dir, ct_path.strip())))\n",
    "            steer = steer\n",
    "        elif camera == 'left':\n",
    "            frame = preprocess(cv2.imread(join(data_dir, lt_path.strip())))\n",
    "            steer = steer + delta_correction\n",
    "        elif camera == 'right':\n",
    "            frame = preprocess(cv2.imread(join(data_dir, rt_path.strip())))\n",
    "            steer = steer - delta_correction\n",
    "\n",
    "        if augment_data:\n",
    "            '''One easy way to get more data is to just flip the image around the horizontal axis \n",
    "            and flip the sign on the angle as well. We instantly get twice the data and we don’t \n",
    "            inadvertently bias the model towards any one direction.\n",
    "            if np.random.randint(2) == 0:\n",
    "                img = np.fliplr(img)\n",
    "                new_angle = -new_angle\n",
    "            '''\n",
    "            # mirror images with chance=0.5\n",
    "            if random.choice([True, False]):\n",
    "                frame = frame[:, ::-1, :]\n",
    "                steer *= -1.\n",
    "                \n",
    "            # perturb slightly steering direction\n",
    "            steer += np.random.normal(loc=0, scale=CONFIG['augmentation_steer_sigma'])\n",
    "            # if color images, randomly change brightness\n",
    "            if CONFIG['input_channels'] == 3:\n",
    "                frame = cv2.cvtColor(frame, code=cv2.COLOR_BGR2HSV)\n",
    "                frame[:, :, 2] *= random.uniform(CONFIG['augmentation_value_min'], CONFIG['augmentation_value_max'])\n",
    "                frame[:, :, 2] = np.clip(frame[:, :, 2], a_min=0, a_max=255)\n",
    "                frame = cv2.cvtColor(frame, code=cv2.COLOR_HSV2BGR)\n",
    "        # check that each element in the batch meet the condition\n",
    "        steer_magnitude_thresh = np.random.rand()\n",
    "        if (abs(steer) + bias) < steer_magnitude_thresh:\n",
    "            pass  # discard this element\n",
    "        else:\n",
    "            X[loaded_elements] = frame\n",
    "            y_steer[loaded_elements] = steer\n",
    "            loaded_elements += 1\n",
    "\n",
    "    if K.backend() == 'theano':\n",
    "        X = X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return X, y_steer\n",
    "\n",
    "\n",
    "def generate_data_batch(data, batchsize=CONFIG['batchsize'], data_dir='data', augment_data=True, bias=0.5):\n",
    "    # set training images resized shape\n",
    "    h, w, c = CONFIG['input_height'], CONFIG['input_width'], CONFIG['input_channels']\n",
    "\n",
    "    while True:\n",
    "        # prepare output structures\n",
    "        X = np.zeros(shape=(batchsize, h, w, c), dtype=np.float32)\n",
    "        y_steer = np.zeros(shape=(batchsize,), dtype=np.float32)\n",
    "        y_throttle = np.zeros(shape=(batchsize,), dtype=np.float32)\n",
    "\n",
    "        # shuffle data\n",
    "        shuffled_data = shuffle(data)\n",
    "\n",
    "        loaded_elements = 0\n",
    "        while loaded_elements < batchsize:\n",
    "\n",
    "            ct_path, lt_path, rt_path, steer, throttle, brake, speed = shuffled_data.pop()\n",
    "\n",
    "            # cast strings to float32\n",
    "            steer = np.float32(steer)\n",
    "            throttle = np.float32(throttle)\n",
    "\n",
    "            # randomly choose which camera to use among (central, left, right)\n",
    "            # in case the chosen camera is not the frontal one, adjust steer accordingly\n",
    "            delta_correction = CONFIG['delta_correction']\n",
    "            camera = random.choice(['frontal', 'left', 'right'])\n",
    "            if camera == 'frontal':\n",
    "                frame = preprocess(cv2.imread(join(data_dir, ct_path.strip())))\n",
    "                steer = steer\n",
    "            elif camera == 'left':\n",
    "                frame = preprocess(cv2.imread(join(data_dir, lt_path.strip())))\n",
    "                steer = steer + delta_correction\n",
    "            elif camera == 'right':\n",
    "                frame = preprocess(cv2.imread(join(data_dir, rt_path.strip())))\n",
    "                steer = steer - delta_correction\n",
    "\n",
    "            if augment_data:\n",
    "\n",
    "                # mirror images with chance=0.5\n",
    "                if random.choice([True, False]):\n",
    "                    frame = frame[:, ::-1, :]\n",
    "                    steer *= -1.\n",
    "\n",
    "                # perturb slightly steering direction\n",
    "                steer += np.random.normal(loc=0, scale=CONFIG['augmentation_steer_sigma'])\n",
    "\n",
    "                # if color images, randomly change brightness\n",
    "                if CONFIG['input_channels'] == 3:\n",
    "                    frame = cv2.cvtColor(frame, code=cv2.COLOR_BGR2HSV)\n",
    "                    frame[:, :, 2] *= random.uniform(CONFIG['augmentation_value_min'], CONFIG['augmentation_value_max'])\n",
    "                    frame[:, :, 2] = np.clip(frame[:, :, 2], a_min=0, a_max=255)\n",
    "                    frame = cv2.cvtColor(frame, code=cv2.COLOR_HSV2BGR)\n",
    "\n",
    "            # check that each element in the batch meet the condition\n",
    "            steer_magnitude_thresh = np.random.rand()\n",
    "            if (abs(steer) + bias) < steer_magnitude_thresh:\n",
    "                pass  # discard this element\n",
    "            else:\n",
    "                X[loaded_elements] = frame\n",
    "                y_steer[loaded_elements] = steer\n",
    "                loaded_elements += 1\n",
    "\n",
    "        if K.backend() == 'theano':\n",
    "            X = X.transpose(0, 3, 1, 2)\n",
    "\n",
    "        yield X, y_steer\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def split_train_val(csv_driving_data):\n",
    "\n",
    "    with open(csv_driving_data, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        driving_data = [row for row in reader][1:]\n",
    "\n",
    "    train_data, val_data = train_test_split(driving_data, test_size=0.2, random_state=1)\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "### Main\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    train_data, val_data = split_train_val(csv_driving_data='data/driving_log.csv')\n",
    "    x_batch, y_batch = load_data_batch(train_data)\n",
    "    nvidia_net = get_nvidia_model(summary=True)\n",
    "    opt = Adam(lr=1e-3)\n",
    "    nvidia_net.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    # json dump of model architecture\n",
    "    with open('logs/model.json', 'w') as f:\n",
    "        f.write(nvidia_net.to_json())\n",
    "\n",
    "    checkpointer = ModelCheckpoint('checkpoints/weights.{epoch:02d}-{val_loss:.3f}.hdf5')\n",
    "    logger = CSVLogger(filename='logs/history.csv')\n",
    "\n",
    "    nvidia_net.fit_generator(generator=generate_data_batch(train_data, augment_data=True, bias=CONFIG['bias']),\n",
    "                         samples_per_epoch=300*CONFIG['batchsize'],\n",
    "                         nb_epoch=50,\n",
    "                         validation_data=generate_data_batch(val_data, augment_data=False, bias=1.0),\n",
    "                         nb_val_samples=100*CONFIG['batchsize'],\n",
    "                         callbacks=[checkpointer, logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here is [VGG keras model](https://github.com/fchollet/keras/blob/master/keras/applications/vgg16.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/keras/blob/master/keras/applications/vgg16.py\n",
    "\"\"\"VGG16 model for Keras.\n",
    "\n",
    "# Reference\n",
    "\n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def preprocess_input(x, dim_ordering='default'):\n",
    "    if dim_ordering == 'default':\n",
    "        dim_ordering = K.image_dim_ordering()\n",
    "    assert dim_ordering in {'tf', 'th'}\n",
    "\n",
    "    if dim_ordering == 'th':\n",
    "        x[:, 0, :, :] -= 103.939\n",
    "        x[:, 1, :, :] -= 116.779\n",
    "        x[:, 2, :, :] -= 123.68\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, ::-1, :, :]\n",
    "    else:\n",
    "        x[:, :, :, 0] -= 103.939\n",
    "        x[:, :, :, 1] -= 116.779\n",
    "        x[:, :, :, 2] -= 123.68\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, :, :, ::-1]\n",
    "    return x\n",
    "\n",
    "\n",
    "def decode_predictions(preds, top=5):\n",
    "    global CLASS_INDEX\n",
    "    if len(preds.shape) != 2 or preds.shape[1] != 1000:\n",
    "        raise ValueError('`decode_predictions` expects '\n",
    "                         'a batch of predictions '\n",
    "                         '(i.e. a 2D array of shape (samples, 1000)). '\n",
    "                         'Found array with shape: ' + str(preds.shape))\n",
    "    if CLASS_INDEX is None:\n",
    "        fpath = get_file('imagenet_class_index.json',\n",
    "                         CLASS_INDEX_PATH,\n",
    "                         cache_subdir='models')\n",
    "        CLASS_INDEX = json.load(open(fpath))\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\n",
    "        results.append(result)\n",
    "    return results\n",
    "#from imagenet_utils import decode_predictions, preprocess_input, _obtain_input_shape\n",
    "\n",
    "\n",
    "TH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5'\n",
    "TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5'\n",
    "TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def VGG16(include_top=True, weights='imagenet',\n",
    "          input_tensor=None, input_shape=None,\n",
    "          classes=1000):\n",
    "    \"\"\"Instantiate the VGG16 architecture,\n",
    "    optionally loading weights pre-trained\n",
    "    on ImageNet. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_dim_ordering=\"tf\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The dimension ordering\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"imagenet\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `tf` dim ordering)\n",
    "            or `(3, 224, 244)` (with `th` dim ordering).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 48.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      dim_ordering=K.image_dim_ordering(),\n",
    "                                      include_top=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n",
    "    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n",
    "    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='vgg16')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if K.image_dim_ordering() == 'th':\n",
    "            if include_top:\n",
    "                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels.h5',\n",
    "                                        TH_WEIGHTS_PATH,\n",
    "                                        cache_subdir='models')\n",
    "            else:\n",
    "                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels_notop.h5',\n",
    "                                        TH_WEIGHTS_PATH_NO_TOP,\n",
    "                                        cache_subdir='models')\n",
    "            model.load_weights(weights_path)\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image dimension ordering convention '\n",
    "                              '(`image_dim_ordering=\"th\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_dim_ordering=\"tf\"` in '\n",
    "                                      ``````````````'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "                convert_all_kernels_in_model(model)\n",
    "        else:\n",
    "            if include_top:\n",
    "                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                        TF_WEIGHTS_PATH,\n",
    "                                        cache_subdir='models')\n",
    "            else:\n",
    "                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                        TF_WEIGHTS_PATH_NO_TOP,\n",
    "                                        cache_subdir='models')\n",
    "            model.load_weights(weights_path)\n",
    "            if K.backend() == 'theano':\n",
    "                convert_all_kernels_in_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
